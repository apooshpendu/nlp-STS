# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FoaU_7MRlQjjYsIuk3qxGsxnvoyqf1Ut
"""

import pandas as pd
import numpy as np
import csv
import random
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
import nltk
import json
import argparse
import pdb
import json
import sys
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

data=pd.read_csv("content/sample_data/msr_paraphrase_data.txt", sep = '\t', quoting=csv.QUOTE_NONE,error_bad_lines=False);

df=data['String']
input=""
for key,value in df.iteritems():
  if key%2==0:
    f=open("content/sample_data/edited.txt", "a+")
    f.write(value+' ')
f.close()
f=open("content/sample_data/edited.txt", "r")

def load_data(file_name):
    sentence_data = []
    with open(file_name, 'r', encoding='utf8') as f:
        data = f.readlines()
    sentence_data = [sent_tokenize(line) for line in data]
    return sentence_data

def get_synonyms_and_antonyms(word):
    synonyms = set()
    antonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join(
                [char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)
            if l.antonyms():
                antonym = l.antonyms()[0].name().replace("_", " ").replace("-", " ").lower()
                antonym = "".join(
                    [char for char in antonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
                antonyms.add(antonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms), list(antonyms)

def synonym_antonym_replacement(sentence_data):
    # sentence_data = load_data(args.file_name)
    result_json = {}
    sw_en = stopwords.words("english")
    for doc_index, document in enumerate(sentence_data):
        result_json[doc_index] = []
        for sentence in document:
            sentence_json = {}
            # just simplely use nltk tools to tokenize and pos_tagging
            words = nltk.word_tokenize(sentence)
            sy_words = words.copy()
            an_words = words.copy()
            pos_list = nltk.pos_tag(words)
            # only output sentence with at least one synonym or antonym
            sy_flag = False
            an_flag = False
            for i in range(len(words)):
                if words[i] not in sw_en and pos_list[i][1] in ["JJ", "RB"]:
                    synonyms, antonyms = get_synonyms_and_antonyms(words[i])
                    # just random pick one synonym and one antonym
                    if len(synonyms) > 0:
                        sy_flag = True
                        synonym = random.sample(synonyms, 1)[0]
                        sy_words[i] = synonym
                    if len(antonyms) > 0:
                        an_flag = True
                        antonym = random.sample(antonyms, 1)[0]
                        an_words[i] = antonym
            sy_sentence = " ".join(sy_words)
            an_sentence = " ".join(an_words)
            if sy_flag:
                sentence_json['synonyms'] = sy_sentence
            if an_flag:
                sentence_json['antonyms'] = an_sentence
            if sy_flag or an_flag:
                sentence_json['raw'] = sentence
                result_json[doc_index].append(sentence_json)
    with open("result.json",'w',encoding = "utf8") as f:
        json.dump(result_json,f,indent = 2,ensure_ascii = False)
    return result_json
    
sentence_data=load_data('content/sample_data/edited.txt')
res=synonym_antonym_replacement(sentence_data)

data1 = pd.read_json('content/result.json')

syn, ant, raw = [],[],[]
for _,row in data1.iterrows():
  temp = row.iloc[0]
  if 'synonyms' in temp:
    syn.append(temp['synonyms'])
  else:
    syn.append(float('nan'))
  if 'antonyms' in temp:
    ant.append(temp['antonyms'])
  else:
    ant.append(float('nan'))
  raw.append(temp['raw'])
data2=data1.copy()
data1['Sentence1']=raw
data1['Sentence2'] = syn
data1['Gold Label']='1'
data2['Sentence1'] = raw
data2['Sentence2'] = ant
data2['Gold Label']='0'

data1 = data1.dropna()
data2 = data2.dropna()

data3=data1.drop(columns=[0])
data4=data2.drop(columns=[0])

data3.reset_index(drop=True,inplace=True)
data4.reset_index(drop=True,inplace=True)

lab = [1] * len(list(data3.Sentence1))

data3['Gold Label'] = lab
data3['is_sts']=[0] * len(list(data3.Sentence1))

lab2 = [0] * len(list(data4.Sentence1))

data4['Gold Label'] = lab2
data4['is_sts']=[0] * len(list(data4.Sentence1))

def generateData(N):
  new = [data3.iloc[0:N//2], data4.iloc[0:N//2]] 
  data_out = pd.concat(new, ignore_index=True)
  data_out.to_csv('syn_ant.csv')   
numlines=int(sys.argv[1])
generateData(numlines)

